{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"top\"></div> \n",
    "# Table of contents\n",
    "* <a href='#Submission-instructions'>Submission instructions</a>\n",
    "* <a href=\"#A-short-introduction-to-LaTeX\">A short introduction to LaTeX</a>\n",
    "* <a href=\"#Some-useful-numpy-functions\">Some useful numpy functions</a>\n",
    "* <a href=\"#The-start-of-this-homework\">The start of this homework</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission instructions\n",
    "Submit your .ipynb file through Sakai\n",
    "\n",
    "Give us your name and PID. List the name and PID of your colaborators. \n",
    "\n",
    "$\\textbf{Nicholas Rewkowski 730047902}$\n",
    "\n",
    "$\\textbf{I had a collaborator who ghosted me before we could do any work so I worked alone}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A short introduction to LaTeX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "$ \\LaTeX $ is a markup language to typeset documents. You can use it to express math compactly and make the layout of your documents beautiful. For the purpose of this class, we focus on the first piece, that is writing mathematical formulations. Jupyter implements a subset of $\\LaTeX$. Therefore, you can use $ \\LaTeX $ to answers the problems in the assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two modes in latex that can typeset the formulations\n",
    "1. Inline mode, start with an \\$, end with an \\$ (\\$...\\$). E.g. $a + b = \\frac{1}{3}$\n",
    "2. Display mode, start with two \\$\\$, end with two \\$\\$ (\\$\\$...\\$\\$). E.g. $$a+b=\\frac{1}{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Maths\n",
    "1. It is straight-forward for basic arithmetic operations. E.g. $+, - * /, a^b, a_b$.\n",
    "2. $ \\LaTeX$ already defined many useful symbols, macros, (or functions) to typeset the formulations. They start with \\, such as \\LaTeX is for the latex symbol. In some typeset functions you can give them parameters. E.g. \\frac{1}{3} typesets one over three, where the thing within the first {} is nominator, and the thing within the second {} is denominator. {} also helps group thing within it together. Consider the difference between a\\_b+1 ($a_b+1$) and a\\_{b+1} ($a_{b+1}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful symbols and functions\n",
    "1. fraction, \\frac{1}{3}. ($\\frac{1}{3}$)\n",
    "2. partial derivative, \\partial. ($\\partial$), Combined with 1), we have \\frac{\\partial f}{\\partial x}. ($\\frac{\\partial f}{\\partial x}$)\n",
    "3. summation, \\sum\\_{i=1}^{N}. ($\\sum_{i=1}^{N}$)\n",
    "4. products, \\prod\\_{i=1}^{N}. ($\\prod_{i=1}^{N}$)\n",
    "5. indexing, w\\_{i, j}. ($w_{i, j}$)\n",
    "6. frequently used Greek letters \\alpha, \\beta, \\gamma. ($\\alpha, \\beta, \\gamma$)\n",
    "7. gradient notation \\nabla. ($\\nabla$)\n",
    "8. vector forms $\\text{\\\\begin{bmatrix} x_{i, 1} \\\\ \\vdots \\\\ x_{i, p+1}  \\\\end{bmatrix}}$. ($\\begin{bmatrix} x_{i, 1} \\\\ \\vdots \\\\ x_{i, p+1} \\end{bmatrix}$) <br \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful numpy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People usualy import numpy as the follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduce some useful numpy functions that you might use in this homework.\n",
    "* zeros <br \\>\n",
    "zeros generates a multi-dimensional array that contains all zeros. <br \\>\n",
    "    * Example: <br \\>\n",
    "    ** zeros( (3, 2) ) ** generates a 2d array of size 3\\*2 (three rows and 2 columns) that contains zeros in all entries. <br \\>\n",
    "    ** zeros( (3, 2, 4) )** generates a 3d array of size 3\\*2\\*4 that also contains zero in all entries.\n",
    "* ones <br \\>\n",
    "Similar to zeros, ones generates a multi-dimensional array that contains all ones. <br \\>\n",
    "* exp <br \\>\n",
    "exp takes exponential on each entry of the input. <br \\>\n",
    "    * Example: <br \\>\n",
    "    ** exp(3) ** computes $ e^3 $. <br \\>\n",
    "    ** exp( [1, 2, 3] ) ** computes $[ e^1, e^2, e^3]$.\n",
    "* log <br \\>\n",
    "Similar to exp, log takes log on each entry of the input. Since log function is not defined on the values $<= 0$, if the input of log is like that, it will output nan or inf defined in numpy. <br \\>\n",
    "* sum <br \\>\n",
    "sum takes sum over an axis of the input array. <br \\>\n",
    "    * Example: please see the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input array is a 3*3 matrix, values: \n",
      " [[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "result 1 is  [ 9 12 15]\n",
      "result 2 is  [ 3 12 21]\n"
     ]
    }
   ],
   "source": [
    "matrixA = np.arange(9).reshape(3, 3)\n",
    "print( 'input array is a 3*3 matrix, values: \\n', matrixA )\n",
    "result1 = np.sum(matrixA, axis = 0)\n",
    "result2 = np.sum(matrixA, axis = 1)\n",
    "print( 'result 1 is ', result1 )\n",
    "print( 'result 2 is ', result2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dot <br \\>\n",
    "do dot-product of the two inputs (if they are vectors), or do matrix multiplication of the two inputs (if they are 2d arrays). In numpy if A and B are both arrays, A\\*B computes the element-wise multiplication, not matrix multiplication. To use dot, the dimensions of the two inputs must be valid. <br \\>\n",
    "    * Example: please see the following code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecotr 1 is a length-3 vector, values:  [1 2 3]\n",
      "vecotr 2 is a length-3 vector, values:  [1 2 1]\n",
      "dot product of vecotr 1 and 2 is 8\n",
      "dot product of vecotr 1 and 3 generates error\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3,) and (4,) not aligned: 3 (dim 0) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-307-787e21c58338>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mvector3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'dot product of vecotr 1 and 3 generates error'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: shapes (3,) and (4,) not aligned: 3 (dim 0) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "#dot product examples\n",
    "vector1 = np.array( [1, 2, 3] )\n",
    "vector2 = np.array( [1, 2, 1] )\n",
    "print( 'vecotr 1 is a length-3 vector, values: ', vector1 )\n",
    "print( 'vecotr 2 is a length-3 vector, values: ', vector2 )\n",
    "print( 'dot product of vecotr 1 and 2 is', np.dot(vector1, vector2) )\n",
    "vector3 = np.array( [1, 2, 1, 2])\n",
    "print( 'dot product of vecotr 1 and 3 generates error' )\n",
    "np.dot(vector1, vector3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix multiplication examples\n",
    "matrix1 = np.arange(6).reshape(2, 3)\n",
    "matrix2 = np.ones(((3,1)))\n",
    "print( 'matrix1 is a 2*3 matrix, values: \\n ', matrix1 )\n",
    "print( 'matrix2 is a 3*1 matrix, values: \\n ', matrix2 )\n",
    "print( 'matrix1 multiplies matrix2 is a 2*1 matrix: \\n ', np.dot(matrix1, matrix2) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* T <br \\>\n",
    "[numpy matrix variable].T, takes transpose of the input matrix variable.\n",
    "    * Example: please see the following code cell <br \\>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.ones( (4, 3) )\n",
    "print( 'matrix dimension is',  matrix1.shape )\n",
    "print( 'matrix dimension after transpose is', matrix1.T.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The start of this homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import plotting utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should take at most 30 seconds\n",
      "Time elapsed (seconds): 0.000993490219116211\n"
     ]
    }
   ],
   "source": [
    "#download data\n",
    "import time\n",
    "import urllib\n",
    "import os.path\n",
    "import sys\n",
    "versionName = sys.version_info\n",
    "if versionName[0] == 2:\n",
    "    import urllib as U\n",
    "elif versionName[0] == 3:\n",
    "    import urllib.request as U\n",
    "start = time.time()\n",
    "print(\"Should take at most 30 seconds\")\n",
    "if not os.path.isfile('train_data.pgz'):\n",
    "    U.urlretrieve(\"https://sakai.unc.edu/access/content/group/c4f84923-328b-429b-a8dc-a340b0284e41/HW1/train_data.pgz\", \"train_data.pgz\")\n",
    "if not os.path.isfile('test_data.pgz'):\n",
    "    U.urlretrieve(\"https://sakai.unc.edu/access/content/group/c4f84923-328b-429b-a8dc-a340b0284e41/HW1/test_data.pgz\", \"test_data.pgz\")\n",
    "if not os.path.isfile('vocab_list.pgz'):\n",
    "    U.urlretrieve( \"https://sakai.unc.edu/access/content/group/c4f84923-328b-429b-a8dc-a340b0284e41/HW1/vocab_list.pgz\", \"vocab_list.pgz\" );\n",
    "end = time.time()\n",
    "print(\"Time elapsed (seconds):\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should take about 15 seconds\n",
      "Time elapsed (seconds): 6.9145026206970215\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "    kwargs = {}\n",
    "except:\n",
    "    import _pickle as pickle\n",
    "    kwargs = {'encoding':'bytes'}\n",
    "    \n",
    "import gzip\n",
    "import numpy as np\n",
    "start = time.time()\n",
    "print(\"Should take about 15 seconds\")\n",
    "train_data, train_label = pickle.load( gzip.open( \"train_data.pgz\", \"rb\" ), **kwargs )\n",
    "train_label = np.asarray(train_label)\n",
    "test_data = pickle.load( gzip.open( \"test_data.pgz\", \"rb\" ),**kwargs )\n",
    "vocab_list = pickle.load( gzip.open( \"vocab_list.pgz\", \"rb\" ),**kwargs )\n",
    "end = time.time()\n",
    "print(\"Time elapsed (seconds):\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'absolutely', 'across', 'act', 'acted', 'acting', 'action', 'actor', 'actors', 'actress', 'actual', 'actually', 'add', 'admit', 'adult', 'adventure', 'age', 'ago', 'agree', 'air', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'amazing', 'america', 'american', 'among', 'amusing', 'animated', 'animation', 'annoying', 'another', 'anyone', 'anything', 'anyway', 'apart', 'apparently', 'appear', 'appears', 'appreciate', 'around', 'art', 'ask', 'atmosphere', 'attempt', 'attempts', 'attention', 'audience', 'average', 'avoid', 'away', 'awful', 'baby', 'back', 'background', 'bad', 'badly', 'band', 'based', 'basic', 'basically', 'battle', 'beautiful', 'beauty', 'became', 'become', 'becomes', 'begin', 'beginning', 'begins', 'behind', 'believable', 'believe', 'ben', 'best', 'better', 'beyond', 'big', 'biggest', 'bill', 'bit', 'bizarre', 'black', 'blood', 'body', 'book', 'books', 'bored', 'boring', 'box', 'boy', 'boys', 'break', 'brilliant', 'bring', 'brings', 'british', 'brother', 'brothers', 'brought', 'budget', 'bunch', 'business', 'buy', 'call', 'called', 'came', 'camera', 'cannot', 'car', 'care', 'career', 'cartoon', 'case', 'cast', 'casting', 'cat', 'caught', 'cause', 'century', 'certain', 'certainly', 'chance', 'change', 'character', 'characters', 'cheap', 'check', 'cheesy', 'child', 'children', 'choice', 'christmas', 'cinema', 'cinematography', 'city', 'class', 'classic', 'clear', 'clearly', 'clever', 'clich', 'close', 'co', 'cold', 'come', 'comedy', 'comes', 'comic', 'coming', 'comment', 'comments', 'common', 'company', 'compared', 'complete', 'completely', 'concept', 'consider', 'considering', 'control', 'convincing', 'cool', 'cop', 'copy', 'could', 'country', 'couple', 'course', 'cover', 'crap', 'crazy', 'create', 'created', 'credit', 'credits', 'creepy', 'crew', 'crime', 'cut', 'cute', 'dance', 'dancing', 'dark', 'daughter', 'david', 'day', 'days', 'de', 'dead', 'deal', 'death', 'decent', 'decided', 'decides', 'deep', 'definitely', 'depth', 'deserves', 'despite', 'development', 'dialog', 'dialogue', 'die', 'died', 'different', 'difficult', 'directed', 'directing', 'direction', 'director', 'directors', 'disappointed', 'disney', 'doctor', 'documentary', 'dog', 'done', 'doubt', 'dr', 'drama', 'dramatic', 'dream', 'due', 'dull', 'dumb', 'dvd', 'earlier', 'early', 'earth', 'easily', 'easy', 'editing', 'effect', 'effective', 'effects', 'effort', 'either', 'elements', 'else', 'emotional', 'end', 'ended', 'ending', 'ends', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'enough', 'entertaining', 'entertainment', 'entire', 'entirely', 'episode', 'episodes', 'era', 'escape', 'especially', 'etc', 'even', 'events', 'eventually', 'ever', 'every', 'everyone', 'everything', 'evil', 'exactly', 'example', 'excellent', 'except', 'exciting', 'expect', 'expected', 'expecting', 'experience', 'extremely', 'eye', 'eyes', 'face', 'fact', 'fails', 'fairly', 'fall', 'falls', 'familiar', 'family', 'famous', 'fan', 'fans', 'fantastic', 'fantasy', 'far', 'fast', 'father', 'favorite', 'fear', 'feature', 'features', 'feel', 'feeling', 'feels', 'felt', 'female', 'fi', 'fight', 'fighting', 'figure', 'filled', 'film', 'filmed', 'filmmakers', 'films', 'final', 'finally', 'find', 'finds', 'fine', 'fire', 'first', 'five', 'flat', 'flick', 'focus', 'follow', 'following', 'follows', 'footage', 'force', 'forced', 'forget', 'form', 'former', 'forward', 'found', 'four', 'free', 'french', 'friend', 'friends', 'front', 'full', 'fun', 'funny', 'future', 'game', 'gave', 'gay', 'general', 'genre', 'george', 'german', 'get', 'gets', 'getting', 'girl', 'girlfriend', 'girls', 'give', 'given', 'gives', 'giving', 'go', 'god', 'goes', 'going', 'gone', 'good', 'gore', 'got', 'great', 'greatest', 'group', 'guess', 'gun', 'guy', 'guys', 'hair', 'half', 'hand', 'hands', 'happen', 'happened', 'happens', 'happy', 'hard', 'hardly', 'hate', 'head', 'hear', 'heard', 'heart', 'hell', 'help', 'hero', 'high', 'highly', 'hilarious', 'history', 'hit', 'hold', 'hollywood', 'home', 'hope', 'horrible', 'horror', 'hot', 'hour', 'hours', 'house', 'however', 'huge', 'human', 'humor', 'husband', 'idea', 'ideas', 'imagine', 'imdb', 'important', 'impressive', 'including', 'incredible', 'incredibly', 'indeed', 'inside', 'instead', 'intelligent', 'interest', 'interested', 'interesting', 'involved', 'island', 'italian', 'jack', 'james', 'jane', 'japanese', 'job', 'joe', 'john', 'joke', 'jokes', 'keep', 'keeps', 'kept', 'kid', 'kids', 'kill', 'killed', 'killer', 'killing', 'kills', 'kind', 'king', 'knew', 'know', 'known', 'knows', 'la', 'lack', 'lady', 'lame', 'language', 'large', 'last', 'late', 'later', 'laugh', 'laughing', 'laughs', 'law', 'lead', 'leading', 'leads', 'learn', 'least', 'leave', 'leaves', 'lee', 'left', 'less', 'let', 'level', 'life', 'light', 'like', 'liked', 'line', 'lines', 'list', 'little', 'live', 'lives', 'living', 'local', 'long', 'look', 'looked', 'looking', 'looks', 'lost', 'lot', 'lots', 'love', 'loved', 'low', 'mad', 'made', 'main', 'major', 'make', 'makes', 'making', 'male', 'man', 'manages', 'many', 'mark', 'married', 'mary', 'masterpiece', 'match', 'material', 'matter', 'may', 'maybe', 'mean', 'means', 'meant', 'meet', 'meets', 'members', 'memorable', 'men', 'mention', 'mentioned', 'mess', 'message', 'michael', 'middle', 'might', 'mind', 'minute', 'minutes', 'miss', 'missed', 'missing', 'modern', 'moment', 'moments', 'money', 'monster', 'mostly', 'mother', 'move', 'moves', 'movie', 'movies', 'moving', 'mr', 'much', 'murder', 'music', 'musical', 'must', 'mystery', 'name', 'named', 'nature', 'near', 'nearly', 'need', 'needed', 'needs', 'neither', 'never', 'new', 'next', 'nice', 'night', 'non', 'none', 'note', 'nothing', 'novel', 'nudity', 'number', 'obvious', 'obviously', 'odd', 'office', 'often', 'oh', 'ok', 'okay', 'old', 'older', 'one', 'ones', 'open', 'opening', 'opinion', 'order', 'original', 'oscar', 'others', 'otherwise', 'outside', 'overall', 'pace', 'parents', 'part', 'particular', 'particularly', 'parts', 'party', 'past', 'paul', 'pay', 'people', 'perfect', 'perfectly', 'performance', 'performances', 'perhaps', 'period', 'person', 'personal', 'peter', 'picture', 'piece', 'place', 'plain', 'planet', 'play', 'played', 'playing', 'plays', 'please', 'plenty', 'plot', 'plus', 'point', 'pointless', 'points', 'police', 'political', 'poor', 'poorly', 'popular', 'portrayal', 'portrayed', 'positive', 'possible', 'possibly', 'potential', 'power', 'powerful', 'predictable', 'premise', 'present', 'pretty', 'previous', 'probably', 'problem', 'problems', 'produced', 'production', 'public', 'pure', 'put', 'quality', 'question', 'quickly', 'quite', 'rate', 'rated', 'rather', 'rating', 'read', 'reading', 'real', 'realistic', 'reality', 'realize', 'really', 'reason', 'reasons', 'recent', 'recently', 'recommend', 'red', 'relationship', 'release', 'released', 'remake', 'remember', 'rent', 'respect', 'rest', 'result', 'return', 'revenge', 'review', 'reviews', 'rich', 'richard', 'ridiculous', 'right', 'robert', 'rock', 'role', 'roles', 'romance', 'romantic', 'room', 'run', 'running', 'runs', 'sad', 'sadly', 'said', 'save', 'saw', 'say', 'saying', 'says', 'scary', 'scene', 'scenes', 'school', 'sci', 'science', 'score', 'scott', 'screen', 'screenplay', 'script', 'season', 'second', 'secret', 'see', 'seeing', 'seem', 'seemed', 'seems', 'seen', 'sees', 'self', 'sense', 'sequel', 'sequence', 'sequences', 'series', 'serious', 'seriously', 'set', 'sets', 'setting', 'several', 'sex', 'sexual', 'shame', 'short', 'shot', 'shots', 'show', 'showing', 'shown', 'shows', 'side', 'silly', 'similar', 'simple', 'simply', 'since', 'singing', 'single', 'sister', 'sit', 'situation', 'slightly', 'slow', 'small', 'social', 'society', 'solid', 'somehow', 'someone', 'something', 'sometimes', 'somewhat', 'son', 'song', 'songs', 'soon', 'sorry', 'sort', 'sound', 'sounds', 'soundtrack', 'space', 'speak', 'special', 'spend', 'spent', 'spirit', 'spoilers', 'stage', 'stand', 'star', 'stars', 'start', 'started', 'starts', 'state', 'stay', 'still', 'stop', 'store', 'stories', 'story', 'storyline', 'straight', 'strange', 'street', 'strong', 'studio', 'stuff', 'stupid', 'style', 'subject', 'success', 'successful', 'suddenly', 'super', 'superb', 'supporting', 'supposed', 'sure', 'surprise', 'surprised', 'suspense', 'sweet', 'take', 'taken', 'takes', 'taking', 'tale', 'talent', 'talented', 'talk', 'talking', 'team', 'television', 'tell', 'telling', 'tells', 'ten', 'tension', 'terrible', 'th', 'theater', 'theme', 'thing', 'things', 'think', 'thinking', 'third', 'though', 'thought', 'three', 'thriller', 'throughout', 'time', 'times', 'title', 'today', 'together', 'told', 'tom', 'tone', 'tony', 'took', 'top', 'total', 'totally', 'towards', 'town', 'trash', 'tried', 'tries', 'trouble', 'true', 'truly', 'truth', 'try', 'trying', 'turn', 'turned', 'turns', 'tv', 'twist', 'two', 'type', 'typical', 'ultimately', 'understand', 'unfortunately', 'unique', 'unless', 'unlike', 'upon', 'us', 'use', 'used', 'uses', 'using', 'usual', 'usually', 'value', 'various', 'version', 'video', 'view', 'viewer', 'viewers', 'viewing', 'villain', 'violence', 'violent', 'visual', 'voice', 'wait', 'waiting', 'walk', 'want', 'wanted', 'wants', 'war', 'waste', 'wasted', 'watch', 'watched', 'watching', 'water', 'way', 'ways', 'weak', 'weird', 'well', 'went', 'western', 'whatever', 'whether', 'white', 'whole', 'whose', 'wife', 'william', 'wish', 'within', 'without', 'woman', 'women', 'wonder', 'wonderful', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worse', 'worst', 'worth', 'would', 'write', 'writer', 'writers', 'writing', 'written', 'wrong', 'wrote', 'year', 'years', 'yes', 'yet', 'york', 'young', 'younger', 'zombie', 'zombies']\n"
     ]
    }
   ],
   "source": [
    "#Consider using small set of the data\n",
    "trainData = train_data[:10000, :]\n",
    "validData = train_data[10000:15000, :]\n",
    "trainLabel = train_label[:10000]\n",
    "validLabel = train_label[10000:15000]\n",
    "testData = test_data[:10000, :]\n",
    "print( vocab_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.01049613e-15, -5.65727465e-16,  2.07511563e-15, -1.25034871e-15,\n",
       "        8.93827234e-16,  2.09658957e-16,  1.24238841e-15,  1.42255097e-16,\n",
       "        1.64055436e-15, -9.47040224e-16,  2.49471777e-15, -2.77537993e-16,\n",
       "       -1.35806033e-15, -2.28569386e-16, -4.80685491e-16,  4.10660395e-17,\n",
       "       -1.47469592e-15, -2.29052333e-16,  1.79992687e-15,  5.72459857e-16,\n",
       "       -7.98441313e-16, -7.43938244e-17, -1.37430067e-15, -4.51683135e-16,\n",
       "        5.87072613e-16, -1.25504496e-15,  7.40705275e-16, -1.24832367e-15,\n",
       "       -2.98550518e-15, -2.53230104e-15, -2.07554862e-15, -1.01518793e-17,\n",
       "        5.15988363e-16, -3.17301740e-16, -1.22530652e-15,  1.63173031e-15,\n",
       "       -1.02405640e-15, -4.67292871e-16, -3.91819910e-17, -1.19720900e-15,\n",
       "        2.47845300e-15, -1.31523681e-16,  6.25703933e-16,  2.51556553e-16,\n",
       "        1.78223658e-15, -1.40591760e-15,  7.31237293e-16, -8.80038264e-16,\n",
       "       -8.96323016e-16,  7.37727657e-16,  8.86091200e-16, -1.26235244e-15,\n",
       "       -4.65816274e-16,  7.35764782e-16, -9.39963662e-16,  3.27660121e-16,\n",
       "       -7.29555305e-16, -7.02549130e-17,  7.68816122e-16,  1.80150561e-15,\n",
       "       -1.61279656e-15,  8.43154435e-16,  1.01290087e-16, -8.83166873e-16,\n",
       "        1.22529320e-15, -1.20171983e-15, -1.47377222e-15, -1.69755321e-16,\n",
       "       -1.13765664e-15, -3.42539108e-15,  1.81495929e-15,  1.02292397e-15,\n",
       "       -5.55755442e-17, -1.03895559e-15, -7.87481191e-17,  7.18989313e-16,\n",
       "        9.81437154e-19,  9.75383108e-16, -7.74407205e-16, -8.23574542e-16,\n",
       "       -1.01402442e-15,  3.20503624e-16, -4.79116746e-16, -1.03097531e-16,\n",
       "        1.40933487e-15, -3.94573263e-16,  4.78002082e-16, -7.11430914e-18,\n",
       "       -2.43287612e-16,  2.28898456e-15, -1.43313361e-15,  1.46509249e-15,\n",
       "       -1.98862482e-15, -1.30762068e-17, -6.47351062e-16,  1.45788936e-16,\n",
       "        1.21128219e-15,  2.37354580e-16,  1.76745174e-15, -1.93071559e-15,\n",
       "        9.48741086e-16,  4.07502920e-16, -9.38817912e-16, -8.92610430e-16,\n",
       "        3.25179883e-16,  2.36988873e-15,  2.25215402e-16,  7.93161092e-16,\n",
       "       -2.62359023e-16, -1.17639232e-15,  5.67770275e-16, -6.75564049e-16,\n",
       "       -1.60262248e-15, -1.10456089e-15, -1.80880422e-15, -1.84725568e-16,\n",
       "        5.26514388e-16, -7.59681207e-17, -1.07724940e-15,  3.15438786e-16,\n",
       "        6.91339208e-16, -8.49844639e-16, -1.53317359e-16, -7.82891529e-16,\n",
       "        2.16185958e-15,  8.82476314e-16,  9.50794998e-17, -6.24167384e-16,\n",
       "        6.38027409e-16, -2.83209012e-16,  5.67299541e-16,  2.11542561e-15,\n",
       "       -1.32543532e-15,  1.21730181e-15, -1.62287073e-15,  2.84945401e-16,\n",
       "       -3.60791397e-16,  3.29047900e-17,  1.50068624e-15,  7.86022358e-16,\n",
       "        1.80736537e-15,  9.44642142e-16, -1.51694879e-15,  5.72326631e-16,\n",
       "        8.80964190e-16, -4.40654180e-16,  7.95490340e-16, -1.13306697e-15,\n",
       "        7.62645502e-16, -3.13096216e-16,  6.70317135e-16, -4.00874889e-16,\n",
       "        2.14348317e-15, -6.71880329e-16, -1.02675646e-16,  7.32527372e-16,\n",
       "       -1.14027232e-15,  4.82573981e-16,  1.19176446e-15, -6.91844360e-16,\n",
       "        3.82704979e-16, -4.89652763e-17, -1.51304747e-15,  2.28078889e-15,\n",
       "       -7.22819582e-16,  2.17898366e-15, -3.08735260e-16, -2.12693863e-15,\n",
       "        1.62317271e-15, -8.82129925e-16, -4.35361081e-15,  8.50559623e-16,\n",
       "       -2.59939625e-15,  7.69695419e-17,  7.42497175e-16, -3.76860765e-16,\n",
       "        5.08189046e-16,  3.69073661e-16, -3.37663231e-17, -3.99542621e-16,\n",
       "       -9.36388744e-16,  1.39301903e-16,  3.04852144e-15, -1.93018268e-15,\n",
       "       -4.64883687e-16, -6.18853857e-16, -1.20092269e-15,  9.00102215e-17,\n",
       "        2.85445001e-16,  6.46807052e-16, -2.77200485e-16,  3.23784111e-15,\n",
       "        1.80306992e-15, -9.08850772e-17, -2.87099011e-15, -1.35207401e-15,\n",
       "       -2.16898277e-15,  3.86011223e-16,  4.66371386e-16, -9.33495503e-16,\n",
       "        2.83064683e-16,  5.12265785e-16,  1.83225879e-15, -7.93938248e-16,\n",
       "        8.40549852e-17, -1.72857062e-15, -3.32640582e-16,  1.23729693e-15,\n",
       "        1.02066799e-15, -1.61660907e-15,  2.23810970e-15, -6.08701978e-16,\n",
       "        8.40421066e-16,  5.26723110e-16,  1.37731160e-15,  5.46973578e-16,\n",
       "       -6.82238710e-16, -1.51072599e-15, -2.85594881e-16,  1.84298299e-15,\n",
       "        5.03018738e-16, -1.25950361e-15, -2.99009706e-16,  1.03612785e-15,\n",
       "        6.08171291e-16, -1.97274419e-15, -1.04659392e-15, -6.35391739e-16,\n",
       "       -1.13860921e-15, -8.10260747e-16,  2.64699374e-16, -5.95927752e-16,\n",
       "        3.98461264e-16, -1.64336322e-15, -1.76401782e-15,  1.05438103e-15,\n",
       "        3.07598391e-17, -5.64719382e-16, -1.31222144e-15,  1.31936906e-15,\n",
       "       -1.87171167e-15, -4.21866986e-16, -1.16513688e-15,  3.51972229e-15,\n",
       "        1.25079502e-15,  6.43143316e-16, -2.81463741e-16,  7.26956273e-16,\n",
       "        9.42057543e-16,  9.05937547e-16,  2.52986521e-16,  1.69237069e-15,\n",
       "        9.67879110e-16,  1.90845117e-15,  1.70089054e-15,  5.67528247e-16,\n",
       "        1.29566802e-15, -8.01847477e-17, -9.83446657e-16,  9.03590536e-16,\n",
       "        1.78297155e-15,  2.50878207e-16, -1.36745726e-15,  7.34110550e-16,\n",
       "       -3.44035911e-17, -5.54194468e-16,  3.23374660e-16,  2.30283792e-15,\n",
       "       -9.75100001e-16, -5.55875346e-16, -8.49529336e-16, -2.10313988e-16,\n",
       "       -2.47004639e-16, -1.34216194e-15,  8.74256223e-16,  2.36015651e-16,\n",
       "       -8.22979462e-16, -4.06519263e-16, -1.96220817e-15,  1.08080211e-16,\n",
       "        1.51324286e-15,  9.38553679e-16, -3.60014241e-16, -5.10065323e-16,\n",
       "       -2.37961872e-15, -1.14221965e-15,  1.04619202e-15, -1.28068001e-15,\n",
       "        2.27775576e-16, -1.63726810e-15, -1.45277346e-15, -2.53934651e-16,\n",
       "        1.91931804e-15, -2.64466227e-16, -2.42852405e-16, -1.76143544e-16,\n",
       "       -1.45716217e-15,  5.09858822e-17,  1.01081810e-15, -1.37288625e-15,\n",
       "       -4.04625222e-16, -4.26707558e-16, -4.44015935e-16, -6.88258339e-16,\n",
       "        3.20263815e-16,  2.27871055e-16, -3.40202755e-15,  3.42141870e-16,\n",
       "       -2.42306175e-16, -8.56514859e-17,  1.62279967e-15, -1.16465282e-15,\n",
       "        5.42597078e-16, -9.43749523e-16,  7.75202125e-17,  8.09790013e-16,\n",
       "       -1.43447920e-15, -1.98935313e-15,  5.66415803e-16, -6.53102017e-16,\n",
       "       -9.67537162e-17, -1.06182618e-15,  7.31630312e-16, -8.17186319e-16,\n",
       "        1.17837740e-15,  1.38239642e-15,  8.42337311e-16, -7.39326378e-16,\n",
       "       -6.85529411e-16, -1.46602730e-15,  5.74535974e-16, -1.84225968e-15,\n",
       "        1.22780786e-15, -4.60879113e-16,  1.19361632e-15,  2.86535240e-16,\n",
       "        9.62385727e-16,  1.64419589e-16,  1.21228805e-15,  1.11285647e-15,\n",
       "       -4.72681894e-16, -1.38143719e-15, -4.10032674e-15,  5.58657565e-16,\n",
       "        6.58195720e-16, -2.35502506e-15, -2.87945223e-16,  5.56732438e-16,\n",
       "        8.97104613e-16,  6.38427977e-15,  1.23118404e-15,  7.23738847e-16,\n",
       "       -2.47034615e-16, -4.90274488e-18, -1.04046771e-15,  9.98705563e-16,\n",
       "       -5.17786924e-16,  2.01687556e-16, -1.73276060e-15,  7.48778817e-16,\n",
       "        3.81792375e-16, -1.60817804e-15,  1.39002365e-15, -1.22469146e-15,\n",
       "        7.67157449e-16,  4.75444129e-16,  1.74609882e-15,  9.62523394e-16,\n",
       "       -2.01025641e-15,  5.29656319e-16, -2.51125343e-15, -1.89397387e-16,\n",
       "       -4.14903667e-16, -7.54230012e-16, -1.80813586e-15,  1.05190967e-15,\n",
       "        1.51519908e-15,  1.44666279e-15, -3.67261777e-16,  1.62605152e-15,\n",
       "       -6.69926337e-16,  5.15720799e-17,  5.64346347e-16, -1.64827929e-15,\n",
       "       -7.23796578e-16, -2.00879313e-16, -1.55557789e-16,  8.37641068e-17,\n",
       "        5.77036197e-16,  7.74333930e-16,  3.24629212e-15,  4.64439598e-16,\n",
       "        4.30380176e-16,  1.88178140e-15, -4.55038229e-16,  3.09205772e-15,\n",
       "       -1.42419410e-15,  6.39313047e-16, -1.20121135e-15,  1.95796712e-16,\n",
       "        1.26051614e-15,  1.36852751e-16, -3.82869292e-16,  2.21473950e-16,\n",
       "       -1.27358790e-15, -4.16287005e-16, -5.68209924e-15, -1.76946235e-15,\n",
       "        1.90158334e-15,  3.31832339e-16,  1.96584971e-16,  1.32619471e-15,\n",
       "       -1.03672626e-17, -7.81243958e-16, -1.02607478e-15,  5.90767435e-16,\n",
       "        8.58220162e-16, -8.40387759e-16, -2.28958408e-15,  1.14912968e-15,\n",
       "        6.69300171e-16,  3.89450694e-16,  1.91666238e-15,  8.94695429e-16,\n",
       "       -2.16034524e-15, -1.06986642e-15, -8.89555096e-16, -7.05824510e-15,\n",
       "        2.22373231e-16,  6.09214901e-16, -1.59909641e-15, -1.00092379e-15,\n",
       "       -1.21928023e-15,  9.96003280e-17,  7.98143773e-16, -3.20383720e-16,\n",
       "       -1.40320422e-15,  1.14073195e-16, -1.19245835e-15,  2.61661248e-15,\n",
       "       -8.59683436e-16, -3.36477513e-16, -2.88017499e-15,  2.33910669e-16,\n",
       "        1.45113699e-15, -2.22970531e-16,  2.81241697e-16, -2.71362044e-15,\n",
       "       -7.16167126e-16,  6.00761663e-16, -3.46194184e-16,  1.05916387e-15,\n",
       "        7.60058683e-18, -5.80957504e-16,  2.11162199e-16, -3.92901267e-16,\n",
       "       -1.05036202e-15, -9.42925737e-16,  4.96735986e-16,  1.31188393e-15,\n",
       "        4.62951899e-16, -4.57354155e-16, -8.15172374e-16,  1.19555033e-15,\n",
       "       -1.18713928e-15,  8.72425465e-16,  8.46549497e-16,  1.63259628e-15,\n",
       "       -1.65449876e-16, -2.32482922e-16,  1.44964041e-16,  1.12997389e-15,\n",
       "        1.16461063e-15,  1.87034832e-16,  7.58570984e-17, -1.59148028e-15,\n",
       "        7.08729742e-16,  7.64206476e-16, -1.33933753e-15, -1.16226140e-15,\n",
       "       -6.96385172e-16, -8.31721358e-16, -3.15669713e-16,  1.03204778e-15,\n",
       "       -4.48713067e-15, -4.47104576e-16,  3.51190188e-16,  3.81921161e-16,\n",
       "       -1.28529853e-15, -3.39968054e-16, -1.01660902e-15,  1.40057743e-15,\n",
       "        1.08048459e-15, -7.29267757e-16, -1.11944232e-15, -1.43718148e-15,\n",
       "       -1.12851506e-15, -4.92599295e-16,  1.50271129e-15,  9.46276391e-16,\n",
       "        1.09838361e-15, -3.72675224e-16,  1.04796172e-15, -1.79962933e-15,\n",
       "       -2.01878958e-15, -4.08428846e-16, -1.09208420e-15, -3.72324394e-17,\n",
       "        2.88049362e-15,  4.66591210e-16,  1.21038957e-15,  4.00370848e-16,\n",
       "        1.03633768e-15, -2.47162291e-16, -1.36179068e-15, -3.39087647e-15,\n",
       "       -1.63911551e-15,  1.75500503e-15, -1.33599798e-16, -1.18780541e-15,\n",
       "       -1.44243284e-15,  5.50011148e-16, -1.31851863e-15,  7.09172721e-16,\n",
       "       -1.79124271e-15, -2.01153538e-16,  1.93948191e-15,  1.78321136e-15,\n",
       "       -1.09379616e-15, -5.62214719e-16,  3.62199160e-17,  1.16454402e-15,\n",
       "        1.16975096e-15,  1.59313229e-15,  4.73725503e-16,  4.82356377e-16,\n",
       "       -1.00860875e-15, -7.86215537e-17, -7.57691687e-16,  2.27989627e-15,\n",
       "       -1.10584208e-15, -7.76596565e-16, -2.05278017e-16, -1.06781251e-17,\n",
       "        3.33555406e-16,  1.27129640e-15,  1.03743236e-15, -9.84470283e-16,\n",
       "        5.51031443e-16, -2.75259815e-16, -5.55548940e-16,  2.62147637e-15,\n",
       "       -7.65607577e-16, -1.97158623e-15, -4.40418813e-16, -1.62436731e-16,\n",
       "       -5.00013364e-16,  6.18908258e-16, -2.78237433e-16,  1.59463553e-16,\n",
       "       -8.10669309e-16,  1.59925406e-15,  5.38702416e-16, -5.32844879e-16,\n",
       "       -1.88827620e-15, -8.24982305e-16,  4.33963976e-17,  7.41338102e-16,\n",
       "       -1.50222057e-16, -7.35118633e-16,  1.90444549e-15,  8.11322121e-16,\n",
       "        2.63892241e-15, -1.85807814e-15,  1.31739064e-17,  1.01789688e-15,\n",
       "        1.46938017e-16, -1.82858173e-16, -4.24904556e-17, -3.92641475e-17,\n",
       "        1.36881395e-15, -2.50485854e-15,  2.39718911e-15, -2.73565615e-15,\n",
       "       -1.97735384e-15,  5.43547429e-16,  6.57525145e-16,  1.09538822e-15,\n",
       "       -4.48323600e-16, -1.66423320e-15,  1.08198006e-15, -1.68041359e-15,\n",
       "       -6.84352575e-16,  2.09742668e-15,  1.26732180e-15, -7.50137730e-16,\n",
       "       -2.24165908e-15, -1.26831878e-17,  3.83499899e-16, -1.08040021e-15,\n",
       "        1.06356257e-15,  1.33883127e-15,  1.00962794e-15,  7.48695550e-17,\n",
       "       -4.75559592e-16, -5.40136824e-16, -1.64961156e-15, -7.45425943e-16,\n",
       "        8.71835937e-17,  1.28401734e-15,  6.19442275e-16,  9.26689836e-16,\n",
       "       -1.34119160e-15,  3.41999762e-16,  6.82545132e-16,  9.48290335e-16,\n",
       "        2.41806575e-18,  1.18979049e-15, -1.00420117e-15, -1.25283561e-15,\n",
       "       -5.44246870e-16, -7.51200213e-16,  6.67814692e-16,  1.33583811e-15,\n",
       "       -7.16937620e-16,  1.02368780e-15, -1.58266955e-15, -1.04375619e-15,\n",
       "        9.05160391e-16,  4.12834211e-16, -9.05011621e-16, -1.73718817e-16,\n",
       "       -1.58068891e-15, -2.53648880e-15,  1.00627284e-15,  6.38162856e-16,\n",
       "        2.86599633e-16, -9.16777765e-17,  3.02535774e-17,  1.06917808e-16,\n",
       "        2.91735525e-16, -3.28341798e-16, -5.23738830e-16, -3.33812977e-16,\n",
       "       -1.21620491e-16, -2.52231569e-16, -7.31015248e-16,  1.37951872e-16,\n",
       "       -1.41607170e-15, -1.01416653e-15, -8.90023610e-16,  1.12805765e-15,\n",
       "       -3.14532844e-16, -1.22214683e-15, -1.01070485e-15, -1.15358390e-15,\n",
       "       -7.26041449e-16, -1.38249856e-15,  3.45347750e-15,  3.38922224e-16,\n",
       "        1.14007692e-15,  6.26778629e-16,  1.19417143e-15, -2.44956722e-15,\n",
       "        1.21798793e-15, -1.89707361e-15, -2.27687202e-15, -1.30718991e-15,\n",
       "        1.00311759e-15,  1.98766781e-15,  1.06439302e-15, -3.01040304e-16,\n",
       "        2.19589458e-15,  1.10560450e-15, -1.01798570e-15, -3.06977332e-15,\n",
       "       -8.22524271e-16, -2.59670063e-16,  1.25355282e-16,  7.04492020e-16,\n",
       "        1.13718812e-15,  2.80078183e-16,  1.64551039e-15, -2.18642882e-16,\n",
       "       -2.51466181e-15, -1.60172098e-15,  3.02755598e-15,  2.20903740e-15,\n",
       "       -1.09747988e-15, -1.59738889e-16,  8.49416093e-16, -4.29913882e-16,\n",
       "       -3.74734022e-15, -1.53650426e-16, -1.79010806e-15, -1.44925405e-15,\n",
       "       -5.49391643e-16, -1.12170162e-15, -5.60218538e-16,  3.29016814e-16,\n",
       "       -1.46159751e-15,  8.63301652e-16, -3.56308316e-16,  1.82002857e-15,\n",
       "        2.51065835e-15,  1.13661969e-15, -1.76380688e-15,  5.83815218e-16,\n",
       "       -5.17070831e-16, -5.21376275e-16, -1.93126404e-15,  9.69251346e-16,\n",
       "       -6.55966392e-16, -1.23568489e-15,  2.45599097e-15, -6.86774193e-15,\n",
       "        6.86500856e-16,  1.63287162e-16,  1.70159664e-15,  2.20597984e-15,\n",
       "        7.91411381e-16, -1.50042201e-16,  1.75524040e-16, -8.47142356e-16,\n",
       "        1.28497879e-15, -4.69668748e-16,  1.36204381e-16,  2.69030576e-15,\n",
       "       -7.01880776e-16,  3.12105897e-16,  9.38471523e-17, -1.47222234e-16,\n",
       "       -3.47037954e-15,  6.02902173e-16,  8.33608738e-16, -2.64663846e-16,\n",
       "        5.77219383e-16,  1.45618739e-15, -6.68601841e-16, -1.00374598e-15,\n",
       "        1.86462401e-15, -1.16314514e-15,  1.59730895e-15,  8.53344062e-16,\n",
       "        5.96656058e-16, -1.04442677e-15, -2.02925454e-16, -8.33786373e-16,\n",
       "        9.24011978e-16,  9.02002917e-16,  1.84334104e-15, -6.03073147e-17,\n",
       "       -1.03340447e-15, -2.83826296e-16, -7.58724195e-16,  2.95710123e-16,\n",
       "        1.08416609e-15, -1.03856035e-15, -1.77489801e-15, -1.05960574e-15,\n",
       "        1.73201453e-16, -6.54776233e-16, -9.27702359e-16, -5.81446002e-16,\n",
       "       -3.40424133e-15,  6.76334544e-16, -2.73867595e-16,  5.23097787e-15,\n",
       "       -2.44375631e-16, -2.24971375e-15,  3.12749826e-17,  4.53173055e-16,\n",
       "       -1.93089988e-16, -7.59179386e-16,  6.82813805e-16, -5.33706412e-16,\n",
       "       -4.21496171e-16,  4.49795756e-17,  1.16467946e-15, -5.82505155e-16,\n",
       "        1.09214859e-16,  2.23773444e-15, -2.71138667e-17, -7.70310482e-16,\n",
       "        4.18356461e-16, -9.36841715e-16,  1.44698586e-15,  1.06401998e-15,\n",
       "        1.04914522e-15, -2.58312260e-15,  7.55642215e-16, -3.10935722e-16,\n",
       "        9.71960290e-16,  2.33257857e-17, -3.64308583e-16,  2.78248535e-16,\n",
       "       -1.63141722e-15,  4.65323335e-16, -4.23623359e-16, -1.02115205e-15,\n",
       "       -6.86162238e-16, -4.75788298e-16,  9.35935773e-16,  9.26907440e-16,\n",
       "        7.00073333e-16,  3.79041243e-16,  7.58149099e-17, -1.67277303e-16,\n",
       "       -2.23763452e-15,  8.38360492e-16,  1.68302039e-15,  1.02019726e-15,\n",
       "        9.48963130e-17,  5.51179102e-16,  3.73008291e-16, -1.38488554e-15,\n",
       "        6.85924650e-16,  3.27184946e-16,  1.05352838e-15,  1.64939395e-15,\n",
       "        1.44929624e-15,  1.56511248e-15,  4.39790426e-16,  1.87741378e-15,\n",
       "       -1.51401114e-16,  2.72324163e-15, -1.42108547e-16,  2.35022668e-15,\n",
       "       -4.60822491e-16, -1.12226672e-15,  1.55883084e-15, -1.58177471e-15,\n",
       "       -2.27113661e-15, -8.98845443e-16, -7.07811587e-17, -2.01738626e-16,\n",
       "       -1.41635592e-16, -1.40502721e-15, -1.10293774e-15, -1.22237553e-15,\n",
       "       -1.43516310e-15, -6.65413280e-16,  4.86695129e-16,  1.13693499e-16,\n",
       "       -3.86939369e-16,  5.41815481e-16,  2.00647277e-15,  1.12564180e-15,\n",
       "        1.14408927e-15,  4.73781014e-16, -9.16053899e-16,  1.15497834e-15,\n",
       "        2.52118326e-16,  1.58177693e-15,  1.40292666e-15,  2.65411471e-15,\n",
       "        1.07579057e-15,  1.59550373e-15,  4.06761291e-16, -2.40285569e-16,\n",
       "        8.04534217e-17,  8.93869423e-16,  6.32200958e-16, -1.40735423e-15,\n",
       "        4.31885638e-16, -3.19140270e-16,  2.27768915e-16,  8.37019343e-16,\n",
       "       -5.26467758e-18,  1.48290269e-16,  2.39102071e-16,  1.58175029e-15,\n",
       "        1.61453517e-15,  2.45592435e-16, -9.89763826e-17,  2.20072849e-16,\n",
       "        2.73541190e-16, -3.11662252e-15,  1.16726406e-15,  2.93984836e-16,\n",
       "        3.19273497e-16,  1.36010758e-15, -2.82966983e-16, -1.18283827e-15,\n",
       "       -3.68651776e-16,  2.52614596e-15, -8.19756485e-16,  4.02665679e-16,\n",
       "       -6.64406308e-16, -1.80826687e-15, -9.33659816e-16, -1.17425181e-15,\n",
       "       -3.69859698e-16, -8.83602080e-16,  5.16653387e-17, -2.62442290e-16,\n",
       "       -2.40951703e-16,  2.25333086e-16, -3.49569262e-16,  5.58291191e-16,\n",
       "        1.95135019e-16, -1.39682488e-15, -1.25318422e-15, -2.10450768e-15,\n",
       "       -4.56377158e-16, -1.10947029e-15, -1.22301502e-15, -6.94431179e-16,\n",
       "        4.87022200e-15, -1.13918652e-15, -6.52899956e-17, -3.75317555e-16,\n",
       "        1.26277877e-15, -7.96785971e-16, -2.44031462e-16,  1.53110857e-16,\n",
       "       -2.63351119e-15,  3.94773103e-17, -4.45332660e-16, -7.33453298e-16,\n",
       "       -1.10809140e-16,  4.00213196e-17, -2.56380028e-15,  4.46653825e-16,\n",
       "        5.79485349e-16,  1.15952137e-15, -3.00082181e-17,  5.23761035e-16,\n",
       "       -9.05469033e-16,  5.40134604e-16, -1.23445476e-15, -2.01105799e-17,\n",
       "       -7.68722863e-16,  1.18407950e-15,  1.15985443e-15, -3.78974629e-16,\n",
       "       -1.32738043e-15,  3.47761819e-16, -3.97466504e-16, -1.50528034e-15,\n",
       "       -5.69277958e-16,  1.46389567e-15, -9.17048659e-16,  8.91813290e-16,\n",
       "        1.92474037e-15,  1.96540784e-15,  1.57257984e-15,  1.32247324e-15,\n",
       "        2.23163710e-16,  2.63908895e-16,  9.42645961e-16, -5.14754905e-17,\n",
       "        2.46147547e-15, -1.07784670e-15,  1.04528608e-15, -1.98841388e-15,\n",
       "        8.34052827e-16, -1.35315759e-15,  4.50237625e-16,  7.30793204e-16,\n",
       "       -4.65065764e-16, -7.37252481e-16,  4.61204408e-16, -2.71648704e-15,\n",
       "        1.02251541e-17,  6.07587314e-16,  1.12117204e-15,  1.64812608e-16,\n",
       "        1.48451917e-15,  1.54324997e-15, -5.17383913e-16,  1.51905155e-16,\n",
       "       -7.62190311e-16,  1.29079858e-15, -2.12947437e-16, -2.10176765e-15,\n",
       "        1.01697983e-15,  1.18683730e-15, -1.02718056e-15,  9.08508824e-16,\n",
       "        7.43349826e-16, -5.92113025e-16,  2.59320121e-15, -6.71107614e-16,\n",
       "        5.94624350e-16,  9.58246815e-16,  1.31591182e-15, -5.07902609e-16,\n",
       "       -2.17807772e-15, -6.39570619e-16, -1.09645626e-15, -6.44924114e-16,\n",
       "        3.87543331e-16,  4.54729587e-16, -6.00821615e-16, -7.77637954e-16,\n",
       "        1.28648203e-16, -5.05835374e-16, -3.03977954e-16, -1.07331033e-15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"top_of_steps\"></div>\n",
    "# Steps\n",
    "1. <a href=\"#Implement-logistic-regression-likelihood.\">Implement logistic regression likelihood.</a>\n",
    "2. <a href=\"#Compute-derivative-of-logistic-regression.\">Compute derivative of logistic regression.</a>\n",
    "3. <a href=\"#Check-gradient.\">Check gradient.</a>\n",
    "4. <a href=\"#Tweak-gradient-ascent-code.\">Tweak gradient ascent code.</a>\n",
    "5. <a href=\"#Report-results-and-analysis.\">Report results and analysis.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement logistic regression likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is given as $D = {(\\mathbf{x}_i, y_i):, i = 1...n}$, where $y_i \\in \\{-1, +1\\}$, and $\\mathbf{x}_i \\in R^p$. In this case there are n samples and each sample has p features. <br \\>\n",
    "\n",
    "For logistic regression, \n",
    "* We have model parameters: $\\mathbf{w} \\in R^p$ for weight and a bias term $b$.\n",
    "* For a sample x and its label y, $p(y|\\mathbf{x}, \\mathbf{w}, b) = \\frac{1}{1+exp\\{-y(\\mathbf{w} \\cdot \\mathbf{x} + b)\\}}$ \n",
    "* We can define $x' = \\begin{bmatrix} 1\\\\ x \\end{bmatrix}$, then $ \\mathbf{w}' =  \\begin{bmatrix} b\\\\ \\mathbf{w} \\end{bmatrix}$. Therefore the bias term is included in the weight vector. For notation brevity, we still use notations $x, \\mathbf{w}$ as $x', \\mathbf{w}'$. This can be implemented by numpy.concatenate function.\n",
    "* Hence the first entry of the vector $w'$ is bias term and the rest are feature weights. In the code you can use w[0] to access the bias term and w[1:] to access the feature weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#We help you do the concatenate, so the first feature becomes the  bias term\n",
    "train_data_pad = np.concatenate( ( np.ones((trainData.shape[0], 1)), trainData ), axis = 1 )\n",
    "test_data_pad = np.concatenate( ( np.ones((testData.shape[0], 1)), testData ), axis = 1 )\n",
    "valid_data_pad = np.concatenate( ( np.ones((validData.shape[0], 1)), validData ), axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-do: \n",
    "1. Given the data $D = {(\\mathbf{x}_i, y_i)} $, write down the likelihood function ($L(\\mathbf{w})$) of logistic regression. ** [1 pt] **\n",
    "2. Take $\\log$ of the likelihood function in (1), write down the log likelihood function. Hint: $\\log$ will not cancel $\\exp$. ** [1 pt] **\n",
    "3. Add  ridge penalty in the log likelihood function (Let the weight of ridge penalty be $\\alpha$). Hint: Do not include $w_0$ in the ridge term. ** [1 pt] **\n",
    "4. Write a function to compute regularized log likelihood ** [1 pt] **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $ L(\\mathbf{w}) =  \\prod_i \\frac{1}{1 + \\exp{\\{-y_i(\\mathbf{w}*\\mathbf{x_i} + b)}\\}}$\n",
    "2. $ LL(\\mathbf{w}) = -\\sum_i \\log\\left\\{1 + \\exp{\\{-y_i(\\mathbf{w}*\\mathbf{x_i} + b)\\}} \\right\\} $\n",
    "3. $ PLL(\\mathbf{w}) = -\\sum_i \\log\\left\\{1 + \\exp{\\{-y_i(\\mathbf{w}*\\mathbf{x_i} + b)\\}} \\right\\}  - \\frac{\\alpha}{2}\\sum_{j=1}^{p} w_j^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loglikelihood(w, X, y, alpha): \n",
    "    #compute loglikelihood for current w, b, given the data X, y\n",
    "    #w is a vector, b is a scalr, X is a n*p matrix and y is a vector.\n",
    "    tmp = 1. + np.exp(-y*(np.dot(w,X.transpose())))\n",
    "    return -np.sum(np.log(tmp)) - (alpha/2.)*np.sum(w[1:]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1808712118395306"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "np.random.seed(1)\n",
    "X = np.random.randn(2,3)\n",
    "y = np.array([1,-1])\n",
    "w = np.ones(3)\n",
    "w[[1]] = -1;\n",
    "loglikelihood(w, X, y, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1808712118395306\n"
     ]
    }
   ],
   "source": [
    "#the values printed in this cell should be the same as the value printed in the previous cell.\n",
    "print( -np.log(1+np.exp(-1*(X[0,0]-X[0,1]+X[0,2]))) - np.log(1+np.exp(1*(X[1,0]-X[1,1]+X[1,2]))) -1/2.*np.sum(w[1:]**2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top_of_steps\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute derivative of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize the function, we want to take the derivative of the function, and update $\\mathbf{w}$ according to the direction of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-do:\n",
    "1. Write down the derivative of the **penalized log likelihood function** for each $ w_j $. Hint: Remember that bias term is $w_0$ and treat it separately from the rest of $w_j$, $j\\in\\{1,...,p\\}$ ** [1 pt] **\n",
    "2. Write down the gradient of log likelihood function. Hint: You can express this in terms of probabilities. ** [1 pt] **\n",
    "3. Update the loglikelihood function to return both the loglikelihood and the gradient. ** [1 pt] **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <br \\>\n",
    "$ \\frac{\\partial PLL(\\mathbf{w})}{ \\partial w_0} = \\sum_i \\frac{y_i \\cdot e^{-{y_i}\\cdot((\\mathbf{w} \\cdot \\mathbf{x_i})+b)}}{1+e^{-y_i\\cdot((\\mathbf{w} \\cdot \\mathbf{x_i})+b)}}$ <br \\>\n",
    "$ \\frac{\\partial PLL(\\mathbf{w})}{ \\partial w_j} = \\sum_i \\frac{y_i \\cdot x_i \\cdot e^{-{y_i}\\cdot((\\mathbf{w} \\cdot \\mathbf{x_i})+b)}}{1+e^{-y_i\\cdot((\\mathbf{w} \\cdot \\mathbf{x_i})+b)}}  - \\alpha w_j, j>0$ <br \\>\n",
    "<br \\>\n",
    "2. <br \\>\n",
    "$ \\nabla PLL(\\mathbf{w}) = \\sum_i \\frac{e^{-{y_i}\\cdot((w \\cdot x_i)+b)}}{1+e^{-y_i\\cdot((w \\cdot x_i)+b)}} \\cdot y_i\\begin{bmatrix} 1 \\\\ x_{i,j}\\\\ \\vdots \\\\  \\end{bmatrix} - \\alpha\\cdot\\begin{bmatrix}  0\\\\ w_j \\\\ \\vdots \\\\  \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loglikelihood(w, X, y, alpha): \n",
    "    #compute loglikelihood for current w, b, given the data X, y\n",
    "    #w is a vector, b is a scalr, X is a n*p matrix and y is a vector.\n",
    "    #h=0\n",
    "    #mysum=0.\n",
    "    #for h in range(len(X)):\n",
    "        #mysum+=np.log(1. + np.exp(-y[h]*(np.dot(w,X[h]))))\n",
    "    tmp = 1. + np.exp(-y*(np.dot(w,X.transpose())))\n",
    "    #mysum=tmp\n",
    "    \n",
    "    \n",
    "    i=0\n",
    "    gradVal=0.\n",
    "    for i in range(len(X)):\n",
    "        #j=0\n",
    "        #newArray=[]\n",
    "        newArray=X[i]\n",
    "        newArray[0]=1.\n",
    "        '''\n",
    "        for j in range(len(X[0])): #over features\n",
    "            if j==0:\n",
    "                newArray.append(1.)\n",
    "            else:\n",
    "                newArray.append(X[i][j])\n",
    "        '''\n",
    "        #print(len(newArray))\n",
    "        divisor=np.exp(np.dot(-y[i],np.dot(w,X[i])))\n",
    "        gradVal+=np.dot(np.dot(divisor/(1.+divisor),y[i]),np.array(newArray))\n",
    "    #gradVal=np.dot(np.sum((1. + np.exp(-np.dot(y,(np.dot(w,X)))))*(1./(1. + np.exp(-np.dot(y,(np.dot(w,X))))))*y),np.array([1,x[1:]]))\n",
    "    #gradVal2=np.sum(gradVal)\n",
    "    #gradVal=np.dot(np.dot(divisor/(1.+divisor),y[i]),np.array(newArray))\n",
    "    #gradVal = np.dot(...,...)\n",
    "    #divisor=np.exp(-y*(np.dot(w,X.transpose())))\n",
    "    #p1=np.dot(divisor/(1.+divisor),y)\n",
    "    #print(p1)\n",
    "    #p2=\n",
    "    #gradVal = np.dot(p1,np.array([0,X[1:]]).transpose())\n",
    "    penaltyArray=alpha*w\n",
    "    penaltyArray[0]=0\n",
    "    '''\n",
    "    k=0\n",
    "    for k in range(len(X[0])):\n",
    "        if k==0:\n",
    "            penaltyArray.append(0.)\n",
    "        else:\n",
    "            penaltyArray.append(alpha*w[k])\n",
    "    '''\n",
    "    penalty = (alpha/2.)*np.sum(w[1:]**2.)\n",
    "    #gradPenalty = - np.dot(alpha,np.array(penaltyArray))\n",
    "    #gradPenalty[0] = 0.; #already 0\n",
    "    #print(gradPenalty)\n",
    "    #for l in range(3):\n",
    "    #    print(gradVal[l],\" \",gradPenalty[l],\" \",gradVal[l] + gradPenalty[l])\n",
    "    #newArray2=[]\n",
    "    #print(len(penaltyArray))\n",
    "    #for l in range(len(X[0])):\n",
    "        #newArray2.append(gradVal[l]-np.array(penaltyArray)[l])\n",
    "    #for m in range(3):\n",
    "    #    print(gradVal[m],\" \",penaltyArray[m],\" \",gradVal[m]-np.array(penaltyArray)[m])\n",
    "    return -np.sum(np.log(tmp)) - penalty, gradVal-np.array(penaltyArray)\n",
    "\n",
    "###################FOR THE RECORD, the assignment specs don't say it needs to be efficient!\n",
    "\n",
    "#g = lambda xy0: loglikelihood(xy0, X=train_data_pad[:,:15], y=trainLabel, alpha=1.)\n",
    "#grad_check( g, w_init[:15], delta=1e-6, tolerance=1e-5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top_of_steps\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important we know the derivative we computed is correctly. We can check it by comparing it with numerical answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load grad_check.py\n",
    "def grad_check(f,xy0,delta=1e-6,tolerance=1e-7):\n",
    "    f0,g0 = f(xy0)\n",
    "    p = len(xy0)\n",
    "    finite_diff = np.zeros(p)\n",
    "    gradient_correct = True\n",
    "    for i in range(p):\n",
    "        xy1 = np.copy(xy0)\n",
    "        xy2 = np.copy(xy0)\n",
    "        xy1[i] = xy1[i] - 0.5*delta\n",
    "        xy2[i] = xy2[i] + 0.5*delta\n",
    "        f1,_ = f(xy1)\n",
    "        f2,_ = f(xy2)\n",
    "        finite_diff = (f2 - f1)/(delta)\n",
    "        if (abs(finite_diff - g0[i])>tolerance):\n",
    "            print(\"Broken partial\",i,\" Finite Diff: \",finite_diff,\" Partial: \",g0[i], \", diff: \",g0[i]-finite_diff)\n",
    "            gradient_correct = False\n",
    "        #else:\n",
    "            #print(\"Broken partial\",i,\" Finite Diff: \",finite_diff,\" Partial: \",g0[i], \", diff: \",g0[i]-finite_diff)\n",
    "    return gradient_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We initialize the w vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = np.random.randn( train_data_pad.shape[1] )*0.001\n",
    "w_init[0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-do:\n",
    "* Here is the code to test if your gradient computation is correct (If the result is true, you get **1 pt**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = lambda xy0: loglikelihood(xy0, X=train_data_pad[:,:15], y=trainLabel, alpha=1)\n",
    "grad_check( g, w_init[:15], delta=1e-6, tolerance=1e-5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top_of_steps\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweak gradient ascent code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide the gradient ascent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# %load gradient_ascent\n",
    "def gradient_ascent(f,x,init_step,iterations):  \n",
    "    f_val,grad = f(x)                           # compute function value and gradient \n",
    "    f_vals = [f_val]\n",
    "    for it in range(iterations):                # iterate for a fixed number of iterations\n",
    "        #print 'iteration %d' % it\n",
    "        done = False                            # initial condition for done\n",
    "        line_search_it = 0                      # how many times we tried to shrink the step\n",
    "        step = init_step                        # reset step size to the initial size\n",
    "        while not done and line_search_it<100:  # are we done yet?\n",
    "            new_x = x + step*grad               # take a step along the gradient\n",
    "            new_f_val,new_grad = f(new_x)       # evaluate function value and gradient\n",
    "            if new_f_val<f_val:                 # did we go too far?\n",
    "                step = step*0.95                # if so, shrink the step-size\n",
    "                line_search_it += 1             # how many times did we shrank the step\n",
    "            else:\n",
    "                done = True                     # better than the last x, so we move on\n",
    "        \n",
    "        if not done:                            # did not find right step size\n",
    "            print(\"Line Search failed.\")\n",
    "        else:\n",
    "            f_val = new_f_val                   # ah, we are ok, accept the new x\n",
    "            x = new_x\n",
    "            grad = new_grad\n",
    "            f_vals.append(f_val)\n",
    "        plt.plot(f_vals)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Function value')\n",
    "    return f_val, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "w_init = np.random.randn( train_data_pad.shape[1] )*0.001\n",
    "w_init[0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-do:\n",
    "* Try different init_step (1e-4, 1e-5, 1e-6) using the following code, report the final regularized log-likelihood values. **[1 pt]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeFn( init_step, iterations, alpha, w):\n",
    "    g = lambda xy0: loglikelihood(xy0, train_data_pad, trainLabel, alpha)\n",
    "    f_val, update_w = gradient_ascent( g, w, init_step, iterations )\n",
    "    return f_val, update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This should take about 6 seconds.\n",
      "Time elapsed (seconds): 11.616922378540039\n",
      "final log-likelihood = -4707.155301\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20f2bc9a128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('This should take about 6 seconds.')\n",
    "start = time.time()\n",
    "f_val, update_w=optimizeFn( init_step = 1e-6, iterations=100, alpha=0, w = w_init) #set init_step to 1e-4, 1e-5, 1e-6\n",
    "end = time.time()\n",
    "print('Time elapsed (seconds):', end-start)\n",
    "print('final log-likelihood = %f\\n' % (f_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final regularized log-likelihood values for (1e-4, 1e-5, 1e-6) are: -2602.028037,-3033.052396,-4707.217753"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top_of_steps\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report results and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the results, we need to have a prediction function, that uses the model we trained to predict the comment is positive and negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-do:\n",
    "1. Implement the prediction function. It should take as inputs feature weights and feature matrix. It should return vector of labels. **[1 pt]**\n",
    "2. Try different alpha (1000, 2000, 3000), and report which alpha produces the model that has the best accuracy on the validation set **[1 pt]**\n",
    "2. **[optional]** Report one sample that is classified wrong with high probabilites (> 90%). **[1 pt]**\n",
    "3. **[optional]** Report the words (entries in vocab_list associated with that feature) that cause the sample reported in (2) classify wrong. Note that weight w[i] correponds to word vocab_list[i-1], because we included bias term in w.**[1 pt]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(w, validData ):\n",
    "    #prob = 1./(1+np.exp((np.dot(w,validData.transpose()))) );\n",
    "    prob = np.exp((np.dot(w,validData.transpose())))/(1. + np.exp((np.dot(w,validData.transpose()))))\n",
    "    #print(prob)\n",
    "    res = np.zeros(validData.shape[0])\n",
    "    #print(\"shape\",validData.shape[0])\n",
    "    #print(\"prob\",prob.shape[0])\n",
    "    res[prob>=0.5] = 1\n",
    "    res[prob<0.5] = -1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on the validation set 84.74%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20f2eae4ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#see the accuracy on the validation set\n",
    "#when init_step=1e-5, the model has the best accuracy in the validation set\n",
    "#print(validLabel)\n",
    "#for i in range(100):\n",
    "   # print(validLabel[i],\" \",validData[i])\n",
    "#print(validData)\n",
    "f_val, update_w=optimizeFn( init_step = 1e-5, iterations=100, alpha=3000, w=w_init) #try different alphas [1000, 2000, 3000]\n",
    "pred = prediction(update_w, valid_data_pad)\n",
    "print( 'accuracy on the validation set {:.2f}%'.format( 100.*np.mean(pred==validLabel)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best alpha is 3000, and the accuracy of this alpha is: 84.74%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Report one sample (sample index in the validation data set) that is classified wrong with high probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_idx = np.nonzero( validLabel != pred )[0] #use this command to get the samples that are predicted wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement the function to compute probability\n",
    "def computeProb(w, validData ):\n",
    "    #prob = 1./(1. + np.exp((np.dot(w,validData.transpose()))) )\n",
    "    prob = np.exp((np.dot(w,validData.transpose())))/(1. + np.exp((np.dot(w,validData.transpose()))))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2027, 2257, 3204]\n"
     ]
    }
   ],
   "source": [
    "#get the samples that are classified wrong and with probabilites > 0.9\n",
    "#print(wrong_idx)\n",
    "#print(\"w1\",len(wrong_idx))\n",
    "#print(wrong_idx_high)\n",
    "#print(\"w2\",len(wrong_idx_high))\n",
    "probs = computeProb(update_w, valid_data_pad)\n",
    "#newprobs=[]\n",
    "wrong_idx_high=[]\n",
    "for i in range(len(wrong_idx)):\n",
    "    if probs[wrong_idx[i]]>=0.9:\n",
    "        wrong_idx_high.append(wrong_idx[i])\n",
    "    #newprobs.append(probs[wrong_idx[i]])\n",
    "#print(\"p1\",len(probs))\n",
    "#newprobs=probs[probs>=0.9]\n",
    "#print(\"p2\",len(newprobs))\n",
    "#wrong_idx_high = wrong_idx[probs>=0.9]\n",
    "print(wrong_idx_high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample index is 2027"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Report the words that cause the sample reported in (2) classify wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this function to get the most important words for each sample index\n",
    "#This function returns a list of top 10 words that influence the prediction.\n",
    "def getMostImportantFeatures( sampleIdx, validData, update_w, vocab_list ):\n",
    "    confusedList = []\n",
    "    intensity = validData[sampleIdx,:]*update_w\n",
    "    #print(intensity.shape)\n",
    "    tmp = np.argsort( np.abs(intensity[0:]) )[::-1]\n",
    "    for j in np.arange(10):\n",
    "        confusedList.append(vocab_list[tmp[j]-1])\n",
    "    return confusedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['favorite',\n",
       " 'songs',\n",
       " 'could',\n",
       " 'great',\n",
       " 'nothing',\n",
       " 'loved',\n",
       " 'perfect',\n",
       " 'completely',\n",
       " 'us',\n",
       " 'think']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusedList = getMostImportantFeatures( 2027, valid_data_pad, update_w, vocab_list) #use the sample index got from the previous result\n",
    "confusedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load file ids\n",
    "if not os.path.isfile('train_id.pgz'):\n",
    "    U.urlretrieve( \"https://sakai.unc.edu/access/content/group/c4f84923-328b-429b-a8dc-a340b0284e41/HW1/train_id.pgz\", \"train_id.pgz\" );\n",
    "train_id = pickle.load( gzip.open( \"train_id.pgz\", \"rb\" ) )\n",
    "valid_id = train_id[10000:15000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Retrieve the whole review and check if it is hard to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9588_4\n"
     ]
    }
   ],
   "source": [
    "fileName = valid_id[2027]\n",
    "print(valid_id[2027])\n",
    "fileUrl = \"https://wwwx.cs.unc.edu/Courses/comp755-f18/hw1/reviews/\" + fileName + '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('9588_4', <http.client.HTTPMessage at 0x20f2ec56470>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.urlretrieve(fileUrl, fileName)\n",
    "\n",
    "####################My answer is yes, it would be hard to classify. It's a movie review about a love story in a racially\n",
    "#tense landscape, so there are clearly mixed signals of positivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a href=\"#top_of_steps\">top</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
